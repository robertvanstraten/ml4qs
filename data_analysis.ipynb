{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration of data features\n",
    "In this notebook we will analyse some of the initial data, by finding out what features are available, what granularity the data comes in, as well as seeing if there is any obvious noise we need to take into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"data/experiments/experiment_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Time (s)' column not found in file: button_presses.csv\n",
      "Columns found: Index(['1686727387', 'break-btn'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Loading in the data from data/experiments/experiment_1, which contains button_presses.csv for the labels, and other csv files for the features. In experiments_1/meta we have the system time and the device info\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def load_data(path, within_range=True, temp_features=True):\n",
    "    # Load the starting time\n",
    "    time_df = pd.read_csv(path + 'meta/time.csv')\n",
    "    start_time = time_df.loc[time_df['event'] == 'START', 'system time'].iloc[0]\n",
    "\n",
    "    data_frames = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path + filename)\n",
    "            # Check if 'Time (s)' column exists\n",
    "            if 'Time (s)' in df.columns:\n",
    "                # Convert 'Time (s)' column to datetime index for each dataframe\n",
    "                df.index = pd.to_datetime(df['Time (s)'], unit='s', origin=pd.Timestamp(start_time, unit='s'))\n",
    "                data_frames.append(df)\n",
    "            else:\n",
    "                print(f\"'Time (s)' column not found in file: {filename}\")\n",
    "                print(f\"Columns found: {df.columns}\")\n",
    "    \n",
    "    # Concatenate dataframes\n",
    "    data = pd.concat(data_frames)\n",
    "    \n",
    "    # resample to 10 Hz\n",
    "    data_resampled = data.resample('100ms').mean()\n",
    "    \n",
    "    # Load label dataset\n",
    "    labels = pd.read_csv(path+'button_presses.csv', names=['Timestamp', 'Label'])\n",
    "    labels['Timestamp'] = pd.to_datetime(labels['Timestamp'], unit='s')\n",
    "    \n",
    "    # Filter timestamps within label range\n",
    "    if within_range:\n",
    "        first_label_timestamp = labels['Timestamp'].iloc[0]\n",
    "        last_label_timestamp = labels['Timestamp'].iloc[-1]\n",
    "        data_resampled = data_resampled[(data_resampled.index >= first_label_timestamp) & (data_resampled.index <= last_label_timestamp)]\n",
    "    \n",
    "    if len(data_resampled):\n",
    "        # Add labels\n",
    "        def get_recent_label(row):\n",
    "            return labels[labels['Timestamp'] <= row.name]['Label'].iloc[-1]\n",
    "\n",
    "        data_resampled['Label'] = data_resampled.apply(get_recent_label, axis=1)\n",
    "\n",
    "        # Add temporal label features\n",
    "        def get_time_until_next(row):\n",
    "            next_label = labels[labels['Timestamp'] > row.name]['Timestamp'].min()\n",
    "            if pd.isnull(next_label):\n",
    "                return pd.NaT\n",
    "            else:\n",
    "                return (next_label - row.name).total_seconds()\n",
    "\n",
    "        def get_time_since_previous(row):\n",
    "            previous_label = labels[labels['Timestamp'] < row.name]['Timestamp'].max()\n",
    "            if pd.isnull(previous_label):\n",
    "                return pd.NaT\n",
    "            else:\n",
    "                return (row.name - previous_label).total_seconds()\n",
    "\n",
    "        data_resampled['Time_Until_Next_Label'] = data_resampled.apply(get_time_until_next, axis=1)\n",
    "        data_resampled['Time_Since_Previous_Label'] = data_resampled.apply(get_time_since_previous, axis=1)\n",
    "        data_resampled['Time_Until_Next_Label'] = data_resampled['Time_Until_Next_Label'].fillna(0.0)\n",
    "        data_resampled['Time_Since_Previous_Label'] = data_resampled['Time_Since_Previous_Label'].fillna(0.0)\n",
    "    return data_resampled\n",
    "\n",
    "data = load_data(experiment)\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a conversion function\n",
    "def convert_timestamp(timestamp):\n",
    "    datetime_obj = datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    return datetime_obj\n",
    "\n",
    "# Load the xml file into a dataframe\n",
    "def load_xml(path, convert_time=True):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(path + 'activity_11340269258.tcx')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the namespaces\n",
    "    namespaces = {\n",
    "        'tc': 'http://www.garmin.com/xmlschemas/TrainingCenterDatabase/v2',\n",
    "        'activity': 'http://www.garmin.com/xmlschemas/TrainingCenterDatabase/v2',\n",
    "        'ns3': 'http://www.garmin.com/xmlschemas/ActivityExtension/v2',\n",
    "        'ns5': 'http://www.garmin.com/xmlschemas/ActivityGoals/v1',\n",
    "        'ns2': 'http://www.garmin.com/xmlschemas/UserProfile/v2',\n",
    "        'xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
    "        'ns4': 'http://www.garmin.com/xmlschemas/ProfileExtension/v1'\n",
    "    }\n",
    "\n",
    "    # Extract data from XML and create a dictionary\n",
    "    xml_data = {'Time': [], 'AltitudeMeters': [], 'HeartRate': []}\n",
    "\n",
    "    for trackpoint in root.findall('.//tc:Trackpoint', namespaces):\n",
    "        time = trackpoint.find('tc:Time', namespaces).text\n",
    "        altitude = trackpoint.find('tc:AltitudeMeters', namespaces).text\n",
    "        heart_rate = trackpoint.find('tc:HeartRateBpm/tc:Value', namespaces).text\n",
    "\n",
    "        xml_data['Time'].append(time)\n",
    "        xml_data['AltitudeMeters'].append(altitude)\n",
    "        xml_data['HeartRate'].append(heart_rate)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(xml_data)\n",
    "    \n",
    "    df['AltitudeMeters'] = df['AltitudeMeters'].astype(float)\n",
    "    df['HeartRate'] = df['HeartRate'].astype(float)\n",
    "    \n",
    "    # Apply the conversion function to the 'Time' column\n",
    "    if convert_time:\n",
    "        df['Time'] = df['Time'].apply(convert_timestamp)\n",
    "    \n",
    "    df = df.set_index('Time')\n",
    "    \n",
    "    return df\n",
    "\n",
    "xml_data = load_xml(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges the csv and xml data\n",
    "def merge(data, xml_data):\n",
    "    first_timestamp = data.index[0]\n",
    "    last_timestamp = data.index[-1]\n",
    "    df_filtered = xml_data[(xml_data.index >= first_timestamp) & (xml_data.index <= last_timestamp)]\n",
    "    merged_df = pd.merge(data, df_filtered, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return merged_df\n",
    "df = merge(data, xml_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change labels to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].astype('category').cat.codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImputationMissingValues import ImputationMissingValues\n",
    "\n",
    "# Interpolates specified columns of the dataframe (all columns with NaNs as default)\n",
    "def interpolate(df, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    imputer = ImputationMissingValues()\n",
    "    for column in columns:\n",
    "        df = imputer.impute_interpolate(df, column)\n",
    "    \n",
    "    return df\n",
    "\n",
    "interpolated_df = interpolate(df)\n",
    "# interpolated_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataTransformation import PrincipalComponentAnalysis\n",
    "\n",
    "# PCA function where number of PC's and columns used can be specified (all columns but the label as default).\n",
    "# Adds new PCA columns to the dataframe\n",
    "def pca(df, num_comp, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.loc[:, ~df.columns.str.startswith('Label')].columns\n",
    "    PCA = PrincipalComponentAnalysis()\n",
    "    pca_df = PCA.apply_pca(df, columns, num_comp)\n",
    "    \n",
    "    return pca_df\n",
    "\n",
    "num_comp = 3\n",
    "pca_df = pca(interpolated_df, num_comp)\n",
    "pca_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LearningAlgorithms import ClassificationAlgorithms\n",
    "\n",
    "X = pca_df.iloc[:, -num_comp:]\n",
    "Y = pca_df.loc[:, pca_df.columns.str.startswith('Label')]\n",
    "split = int(0.8 * len(pca_df))\n",
    "\n",
    "clf = ClassificationAlgorithms()\n",
    "pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y = clf.support_vector_machine_with_kernel(X[:split], Y[:split], X[split:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def plot_data(data, output_path):\n",
    "    # Set Seaborn style and context\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "\n",
    "    # Line plot\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(data=data, palette=\"tab10\", linewidth=1.5, ax=ax)\n",
    "    ax.set_title('Time Series Line Plot')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Values')\n",
    "    fig.savefig(output_path + 'line_plot.pdf')\n",
    "\n",
    "    # Distribution of the data\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.kdeplot(data=data, ax=ax, fill=True)\n",
    "    ax.set_title('Data Distribution Plot')\n",
    "    ax.set_xlabel('Values')\n",
    "    fig.savefig(output_path + 'distribution_plot.pdf')\n",
    "\n",
    "    # Boxplots of the data\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(data=data, palette=\"tab10\", ax=ax)\n",
    "    ax.set_title('Data Boxplot')\n",
    "    fig.savefig(output_path + 'boxplot.pdf')\n",
    "\n",
    "    # Correlation matrix of the data\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='coolwarm', ax=ax)\n",
    "    ax.set_title('Data Correlation Matrix')\n",
    "    fig.savefig(output_path + 'correlation_matrix.pdf')\n",
    "\n",
    "    # Pairwise relationships\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.pairplot(data)\n",
    "    ax.set_title('Pairwise Relationships')\n",
    "    fig.savefig(output_path + 'pairwise_relationships.pdf')\n",
    "\n",
    "    # Histogram\n",
    "    fig, ax = plt.subplots()\n",
    "    data.hist(bins=30, ax=ax)\n",
    "    ax.set_title('Data Histogram')\n",
    "    fig.savefig(output_path + 'histogram.pdf')\n",
    "\n",
    "    # Time series decomposition\n",
    "    for col in data.columns:\n",
    "        try:\n",
    "            result = seasonal_decompose(data[col], model='additive', period=1)\n",
    "            fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(10,8))\n",
    "            result.observed.plot(ax=ax1)\n",
    "            ax1.set_ylabel('Observed')\n",
    "            result.trend.plot(ax=ax2)\n",
    "            ax2.set_ylabel('Trend')\n",
    "            result.seasonal.plot(ax=ax3)\n",
    "            ax3.set_ylabel('Seasonal')\n",
    "            result.resid.plot(ax=ax4)\n",
    "            ax4.set_ylabel('Residual')\n",
    "            fig.savefig(output_path + f'{col}_decomposition.pdf')\n",
    "        except:\n",
    "            print(f\"Cannot decompose {col}\")\n",
    "\n",
    "output_path = experiment + \"figures/\"\n",
    "plot_data(df, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
