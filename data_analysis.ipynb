{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration of data features\n",
    "In this notebook we will analyse some of the initial data, by finding out what features are available, what granularity the data comes in, as well as seeing if there is any obvious noise we need to take into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"data/experiments/experiment_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Loading in the data from data/experiments/experiment_1, which contains button_presses.csv for the labels, and other csv files for the features. In experiments_1/meta we have the system time and the device info\n",
    "def load_data(path, within_range=True, temp_features=True):\n",
    "    # Load the starting time\n",
    "    time_df = pd.read_csv(path + 'meta/time.csv')\n",
    "    start_time = time_df.loc[time_df['event'] == 'START', 'system time'].iloc[0]\n",
    "\n",
    "    data_frames = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path + filename)\n",
    "            # Check if 'Time (s)' column exists\n",
    "            if 'Time (s)' in df.columns:\n",
    "                # Convert 'Time (s)' column to datetime index for each dataframe\n",
    "                df.index = pd.to_datetime(df['Time (s)'], unit='s', origin=pd.Timestamp(start_time, unit='s'))\n",
    "                data_frames.append(df)\n",
    "            else:\n",
    "                print(f\"'Time (s)' column not found in file: {filename}\")\n",
    "                print(f\"Columns found: {df.columns}\")\n",
    "    \n",
    "    # Concatenate dataframes\n",
    "    data = pd.concat(data_frames)\n",
    "    \n",
    "    # resample to 10 Hz\n",
    "    data_resampled = data.resample('100ms').mean()\n",
    "    \n",
    "    # Load label dataset\n",
    "    labels = pd.read_csv(path+'button_presses.csv', names=['Timestamp', 'Label'])\n",
    "    labels['Timestamp'] = pd.to_datetime(labels['Timestamp'], unit='s')\n",
    "    \n",
    "    # Filter timestamps within label range\n",
    "    if within_range:\n",
    "        first_label_timestamp = labels['Timestamp'].iloc[0]\n",
    "        last_label_timestamp = labels['Timestamp'].iloc[-1]\n",
    "        data_resampled = data_resampled[(data_resampled.index >= first_label_timestamp) & (data_resampled.index <= last_label_timestamp)]\n",
    "    \n",
    "    if len(data_resampled):\n",
    "        # Add labels\n",
    "        def get_recent_label(row):\n",
    "            return labels[labels['Timestamp'] <= row.name]['Label'].iloc[-1]\n",
    "\n",
    "        data_resampled['Label'] = data_resampled.apply(get_recent_label, axis=1)\n",
    "\n",
    "        # Add temporal label features\n",
    "        def get_time_until_next(row):\n",
    "            next_label = labels[labels['Timestamp'] > row.name]['Timestamp'].min()\n",
    "            if pd.isnull(next_label):\n",
    "                return pd.NaT\n",
    "            else:\n",
    "                return (next_label - row.name).total_seconds()\n",
    "\n",
    "        def get_time_since_previous(row):\n",
    "            previous_label = labels[labels['Timestamp'] < row.name]['Timestamp'].max()\n",
    "            if pd.isnull(previous_label):\n",
    "                return pd.NaT\n",
    "            else:\n",
    "                return (row.name - previous_label).total_seconds()\n",
    "\n",
    "        data_resampled['Time_Until_Next_Label'] = data_resampled.apply(get_time_until_next, axis=1)\n",
    "        data_resampled['Time_Since_Previous_Label'] = data_resampled.apply(get_time_since_previous, axis=1)\n",
    "        data_resampled['Time_Until_Next_Label'] = data_resampled['Time_Until_Next_Label'].fillna(0.0)\n",
    "        data_resampled['Time_Since_Previous_Label'] = data_resampled['Time_Since_Previous_Label'].fillna(0.0)\n",
    "    return data_resampled\n",
    "\n",
    "data = load_data(experiment)\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a conversion function\n",
    "def convert_timestamp(timestamp):\n",
    "    datetime_obj = datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    return datetime_obj\n",
    "\n",
    "# Load the xml file into a dataframe\n",
    "def load_xml(path, convert_time=True):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(path + 'activity_11340269258.tcx')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the namespaces\n",
    "    namespaces = {\n",
    "        'tc': 'http://www.garmin.com/xmlschemas/TrainingCenterDatabase/v2',\n",
    "        'activity': 'http://www.garmin.com/xmlschemas/TrainingCenterDatabase/v2',\n",
    "        'ns3': 'http://www.garmin.com/xmlschemas/ActivityExtension/v2',\n",
    "        'ns5': 'http://www.garmin.com/xmlschemas/ActivityGoals/v1',\n",
    "        'ns2': 'http://www.garmin.com/xmlschemas/UserProfile/v2',\n",
    "        'xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
    "        'ns4': 'http://www.garmin.com/xmlschemas/ProfileExtension/v1'\n",
    "    }\n",
    "\n",
    "    # Extract data from XML and create a dictionary\n",
    "    xml_data = {'Time': [], 'AltitudeMeters': [], 'HeartRate': []}\n",
    "\n",
    "    for trackpoint in root.findall('.//tc:Trackpoint', namespaces):\n",
    "        time = trackpoint.find('tc:Time', namespaces).text\n",
    "        altitude = trackpoint.find('tc:AltitudeMeters', namespaces).text\n",
    "        heart_rate = trackpoint.find('tc:HeartRateBpm/tc:Value', namespaces).text\n",
    "\n",
    "        xml_data['Time'].append(time)\n",
    "        xml_data['AltitudeMeters'].append(altitude)\n",
    "        xml_data['HeartRate'].append(heart_rate)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(xml_data)\n",
    "    \n",
    "    df['AltitudeMeters'] = df['AltitudeMeters'].astype(float)\n",
    "    df['HeartRate'] = df['HeartRate'].astype(float)\n",
    "    \n",
    "    # Apply the conversion function to the 'Time' column\n",
    "    if convert_time:\n",
    "        df['Time'] = df['Time'].apply(convert_timestamp)\n",
    "    \n",
    "    df = df.set_index('Time')\n",
    "    \n",
    "    return df\n",
    "\n",
    "xml_data = load_xml(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merges the csv and xml data\n",
    "def merge(data, xml_data):\n",
    "    first_timestamp = data.index[0]\n",
    "    last_timestamp = data.index[-1]\n",
    "    df_filtered = xml_data[(xml_data.index >= first_timestamp) & (xml_data.index <= last_timestamp)]\n",
    "    merged_df = pd.merge(data, df_filtered, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return merged_df\n",
    "df = merge(data, xml_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframe from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(experiment + 'merged/added_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change labels to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unuseful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(df, columns):\n",
    "    return df.drop(columns=columns)\n",
    "\n",
    "# 'X (hPa)' nog evt\n",
    "columns = ['Velocity (m/s)', 'Direction (°)', 'Distance (cm)', 'Horizontal Accuracy (m)', 'Time_Until_Next_Label', 'Time_Since_Previous_Label', 'Time (s)', 'Latitude (°)', 'Longitude (°)']\n",
    "df_filt = df.drop(columns=columns)\n",
    "df_filt = df_filt.rename(columns={'Time (s).1': 'Time (s)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_values(df):\n",
    "    exclude_columns = ['Time (s)', 'Label']  # Add a comma between the column names\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df.loc[:, ~df.columns.isin(exclude_columns)])\n",
    "    scaled_data = scaler.transform(df.loc[:, ~df.columns.isin(exclude_columns)])\n",
    "    df.loc[:, ~df.columns.isin(exclude_columns)] = scaled_data\n",
    "    return df\n",
    "\n",
    "scaled_df = scale_values(df_filt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImputationMissingValues import ImputationMissingValues\n",
    "\n",
    "# Interpolates specified columns of the dataframe (all columns with NaNs as default)\n",
    "def interpolate(df, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    imputer = ImputationMissingValues()\n",
    "    for column in columns:\n",
    "        df = imputer.impute_interpolate(df, column)\n",
    "    \n",
    "    return df\n",
    "\n",
    "interpolated_df = interpolate(scaled_df)\n",
    "# interpolated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features\n",
    "Mean, std, min, max, difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, seconds, columns=None):\n",
    "    # Create a new DataFrame to store the aggregated values\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    # Define the window size\n",
    "    window_size = seconds * 10\n",
    "    \n",
    "    if columns == None:\n",
    "        columns = df.loc[:, ~df.columns.str.startswith('Label')].columns\n",
    "\n",
    "    # Iterate over the rolling windows in the original DataFrame\n",
    "    for i in range(len(df) - window_size + 1):\n",
    "        # Select a rolling window subset\n",
    "        subset = df.iloc[i:i+window_size]\n",
    "\n",
    "        # Iterate over each column in the subset\n",
    "        for col in columns:\n",
    "            if col == \"Time (s)\":\n",
    "                new_df.loc[i, f'Starttime (s)'] = subset[col].iloc[0]\n",
    "                new_df.loc[i, f'Endtime (s)'] = subset[col].iloc[-1]\n",
    "            \n",
    "            else:\n",
    "                col_mean = subset[col].mean()\n",
    "                col_std = subset[col].std()\n",
    "                col_min = subset[col].min()\n",
    "                col_max = subset[col].max()\n",
    "                col_diff = subset[col].iloc[-1] - subset[col].iloc[0]\n",
    "\n",
    "                # Create new columns in the new DataFrame\n",
    "                new_df.loc[i, f'{col} mean'] = col_mean\n",
    "                new_df.loc[i, f'{col} std'] = col_std\n",
    "                new_df.loc[i, f'{col} min'] = col_min\n",
    "                new_df.loc[i, f'{col} max'] = col_max\n",
    "                new_df.loc[i, f'{col} diff'] = col_diff\n",
    "\n",
    "        # Get the most frequent label within the window\n",
    "        most_frequent_label = subset['Label'].mode().iloc[0]\n",
    "        new_df.loc[i, 'Label'] = most_frequent_label\n",
    "\n",
    "    # Reset the index of the new DataFrame\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print the new DataFrame\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full = create_features(interpolated_df, 5, columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robbie vanaf hier runne!! ga nu renne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(experiment + 'merged/added_features_stats_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase samples of minority classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_full.drop(['Label'], axis=1)\n",
    "labels = df_full['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = df_full['Label'].astype('category').cat.codes\n",
    "\n",
    "# Create a dictionary to store the mapping of encoded labels to original words\n",
    "label_mapping = dict(zip(encoded_labels, df_full['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['Break', 'Falling', 'Lowering', 'Overhanging', 'Slab', 'Straight']\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(6,5))\n",
    "label_counts = np.bincount(df_full['Label'].astype('category').cat.codes)\n",
    "x = np.arange(len(class_labels))\n",
    "plt.bar(x, label_counts, edgecolor='black')\n",
    "plt.xticks(range(len(class_labels)), class_labels)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Encoded Labels')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"labels_original.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = labels.value_counts()\n",
    "\n",
    "# Calculate the ratios between labels\n",
    "label_ratios = label_counts / label_counts.sum()\n",
    "\n",
    "# Print the label ratios\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second most frequent label must become 60% of the most frequent label\n",
    "# All other labels scale accordingly\n",
    "minority_ratio = 0.6 * max(label_counts)/sorted(label_counts)[-2]\n",
    "\n",
    "# Define custom resampling ratios for each label\n",
    "resampling_ratios = {\n",
    "    0: label_counts[0],  # Resampling ratio for label 0\n",
    "    1: int(minority_ratio * label_counts[1]),   # Resampling ratio for label 1\n",
    "    2: int(minority_ratio * label_counts[2]),   # Resampling ratio for label 2\n",
    "    3: int(minority_ratio * label_counts[3]),   # Resampling ratio for label 3\n",
    "    4: int(minority_ratio * label_counts[4]),   # Resampling ratio for label 4\n",
    "    5: int(minority_ratio * label_counts[5])    # Resampling ratio for label 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Create an instance of SMOTE\n",
    "smote = SMOTE(sampling_strategy=resampling_ratios)\n",
    "\n",
    "# Resample the data using SMOTE\n",
    "features_resampled, labels_resampled = smote.fit_resample(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(6,5))\n",
    "label_counts = np.bincount(labels_resampled)\n",
    "x = np.arange(len(class_labels))\n",
    "plt.bar(x, label_counts, edgecolor='black')\n",
    "plt.xticks(range(len(class_labels)), class_labels)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Encoded Labels')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"labels_sampled.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataTransformation import PrincipalComponentAnalysis\n",
    "\n",
    "# PCA function where number of PC's and columns used can be specified (all columns but the label as default).\n",
    "# Adds new PCA columns to the dataframe\n",
    "def pca(df, num_comp, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    PCA = PrincipalComponentAnalysis()\n",
    "    pca_df = PCA.apply_pca(df, columns, num_comp)\n",
    "    \n",
    "    return pca_df\n",
    "\n",
    "def determine_pc_explained_variance(df, columns = None):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    PCA = PrincipalComponentAnalysis()\n",
    "    ratio, feature_importances = PCA.determine_pc_explained_variance(df, columns)\n",
    "    \n",
    "    return ratio, feature_importances\n",
    "\n",
    "ratio, feature_importances = determine_pc_explained_variance(features_resampled, columns = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_features = len(feature_importances)\n",
    "# n_features = 25\n",
    "\n",
    "# feature_names = features_resampled.columns  # Provide the names of your original features\n",
    "\n",
    "# sorted_indices = sorted(range(len(feature_importances)), key=lambda k: feature_importances[k], reverse=True)\n",
    "# sorted_feature_importances = [feature_importances[i] for i in sorted_indices]\n",
    "# sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.bar(range(n_features), sorted_feature_importances[:25], tick_label=sorted_feature_names[:25])\n",
    "# plt.xticks(rotation=90, fontsize=9)\n",
    "# plt.xlabel('Features')\n",
    "# plt.ylabel('Importance')\n",
    "# plt.title('Feature Importances with PCA')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance = np.cumsum(ratio)\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(\"Number of components to keep:\", n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_df = pca(features_resampled, n_components).copy()\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LearningAlgorithms import ClassificationAlgorithms\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "X = pca_df.iloc[:, -n_components:]\n",
    "Y = labels_resampled\n",
    "\n",
    "one = int(0.10 * len(X))\n",
    "two = int(0.30 * len(X))\n",
    "\n",
    "X_train = pd.concat([X[:one], X[two:]], axis=0)\n",
    "X_test = X[one:two]\n",
    "Y_train = pd.concat([Y[:one], Y[two:]], axis=0)\n",
    "Y_test = Y[one:two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = ClassificationAlgorithms()\n",
    "# pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y = clf.support_vector_machine_with_kernel(X_train, Y_train, X_test, C=100,  kernel='rbf', gamma=0.0001, gridsearch=False, print_model_details=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(experiment + 'svm/pred_test_y3', 'rb') as file:\n",
    "    pred_test_y = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check label distribution in train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_train, edgecolor='black')\n",
    "plt.xlabel('Encoded Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Encoded Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_test, edgecolor='black')\n",
    "plt.xlabel('Encoded Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Encoded Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def metrics(labels, predictions):\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    classes = np.arange(6)\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.savefig(\"confusion_matrix.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "metrics(Y_test, pred_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from scipy import stats\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pandas.plotting import register_matplotlib_converters\n",
    "# register_matplotlib_converters()\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# def plot_data(data, output_path):\n",
    "#     # Set Seaborn style and context\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.set_context(\"paper\")\n",
    "\n",
    "#     # Line plot\n",
    "#     fig, ax = plt.subplots()\n",
    "#     sns.lineplot(data=data, palette=\"tab10\", linewidth=1.5, ax=ax)\n",
    "#     ax.set_title('Time Series Line Plot')\n",
    "#     ax.set_xlabel('Time')\n",
    "#     ax.set_ylabel('Values')\n",
    "#     fig.savefig(output_path + 'line_plot.pdf')\n",
    "\n",
    "#     # Distribution of the data\n",
    "#     fig, ax = plt.subplots()\n",
    "#     sns.kdeplot(data=data, ax=ax, fill=True)\n",
    "#     ax.set_title('Data Distribution Plot')\n",
    "#     ax.set_xlabel('Values')\n",
    "#     fig.savefig(output_path + 'distribution_plot.pdf')\n",
    "\n",
    "#     # Boxplots of the data\n",
    "#     fig, ax = plt.subplots()\n",
    "#     sns.boxplot(data=data, palette=\"tab10\", ax=ax)\n",
    "#     ax.set_title('Data Boxplot')\n",
    "#     fig.savefig(output_path + 'boxplot.pdf')\n",
    "\n",
    "#     # Correlation matrix of the data\n",
    "#     fig, ax = plt.subplots()\n",
    "#     sns.heatmap(data.corr(), annot=True, cmap='coolwarm', ax=ax)\n",
    "#     ax.set_title('Data Correlation Matrix')\n",
    "#     fig.savefig(output_path + 'correlation_matrix.pdf')\n",
    "\n",
    "#     # Pairwise relationships\n",
    "#     fig, ax = plt.subplots()\n",
    "#     sns.pairplot(data)\n",
    "#     ax.set_title('Pairwise Relationships')\n",
    "#     fig.savefig(output_path + 'pairwise_relationships.pdf')\n",
    "\n",
    "#     # Histogram\n",
    "#     fig, ax = plt.subplots()\n",
    "#     data.hist(bins=30, ax=ax)\n",
    "#     ax.set_title('Data Histogram')\n",
    "#     fig.savefig(output_path + 'histogram.pdf')\n",
    "\n",
    "#     # Time series decomposition\n",
    "#     for col in data.columns:\n",
    "#         try:\n",
    "#             result = seasonal_decompose(data[col], model='additive', period=1)\n",
    "#             fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(10,8))\n",
    "#             result.observed.plot(ax=ax1)\n",
    "#             ax1.set_ylabel('Observed')\n",
    "#             result.trend.plot(ax=ax2)\n",
    "#             ax2.set_ylabel('Trend')\n",
    "#             result.seasonal.plot(ax=ax3)\n",
    "#             ax3.set_ylabel('Seasonal')\n",
    "#             result.resid.plot(ax=ax4)\n",
    "#             ax4.set_ylabel('Residual')\n",
    "#             fig.savefig(output_path + f'{col}_decomposition.pdf')\n",
    "#         except:\n",
    "#             print(f\"Cannot decompose {col}\")\n",
    "\n",
    "# output_path = experiment + \"figures/\"\n",
    "# plot_data(df, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
